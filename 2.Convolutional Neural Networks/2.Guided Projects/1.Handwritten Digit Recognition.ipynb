{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/Deep-Learning-Challenge/challenge-notebooks/blob/master/2.Convolutional%20Neural%20Networks/2.Guided%20Projects/1.Handwritten%20Digit%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handwritten Digit Recognition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A popular demonstration of the capability of deep learning techniques is object recognition in image data. The hello world of object recognition for machine learning and deep learning is the MNIST dataset for handwritten digit recognition. In this project, you will discover how to develop a deep learning model to achieve near state-of-the-art performance on the MNIST handwritten digit recognition task in Python using the Keras deep learning library. After completing this step-by-step tutorial, you will know:\n",
    "* How to load the MNIST dataset in Keras and develop a baseline neural network model for the problem.\n",
    "* How to implement and evaluate a simple Convolutional Neural Network for MNIST.\n",
    "* How to implement a close to state-of-the-art deep learning model for MNIST.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "**Note**: You may want to speed up the tutorial's computation using GPU rather than CPU hardware. This is a suggestion, not a requirement. The tutorial will work just fine on the CPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handwritten Digit Recognition Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MNIST problem is a dataset developed by Yann LeCun, Corinna Cortes, and Christopher Burges for evaluating machine learning models on the handwritten [digit classification problem](http://yann.lecun.com/exdb/mnist/). The dataset was constructed from a number of scanned document datasets available from the National Institute of Standards and Technology (NIST). This is where the name for the dataset comes from, as the Modified NIST or MNIST dataset.\n",
    "\n",
    "Images of digits were taken from a variety of scanned documents, normalized in size and centered. This makes it an excellent dataset for evaluating models, allowing the developer to focus on the machine learning with minimal data cleaning or preparation required. Each image is a 28 x 28-pixel square (784 pixels total). A standard split of the dataset is used to evaluate and compare models, where 60,000 images are used to train a model, and a separate set of 10,000 images are used to test it.\n",
    "\n",
    "It is a digit recognition task. As such, there are ten digits (0 to 9) or ten classes to predict. Results are reported using prediction error, which is nothing more than the inverted classification accuracy. Excellent results achieve a prediction error of less than 1%. A state-of-the-art prediction error of approximately 0.2% can be achieved with large Convolutional Neural Networks. There is a listing of the state-of-the-art results and links to the relevant papers on the MNIST and other datasets on [Rodrigo Benenson's webpage](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the MNIST dataset in Keras"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Keras deep learning library provides a convenient method for loading the MNIST dataset. The dataset is downloaded automatically the first time this function is called and is stored in your home directory in `~/.keras/datasets/mnist.pkl.gz` as a 15 megabyte file. This is very\n",
    "handy for developing and testing deep learning models. To demonstrate how easy it is to load the MNIST dataset, we will first write a little script to download and visualize the first four images in the training dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Plot ad hoc mnist instances\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"250.552409pt\" version=\"1.1\" viewBox=\"0 0 315.579545 250.552409\" width=\"315.579545pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-11-05T23:58:21.740562</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 250.552409 \nL 315.579545 250.552409 \nL 315.579545 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 108.070647 \nL 125.761364 108.070647 \nL 125.761364 9.234284 \nL 26.925 9.234284 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pfbfef13a95)\">\n    <image height=\"99\" id=\"image863fe544d0\" transform=\"scale(1 -1)translate(0 -99)\" width=\"99\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAGMAAABjCAYAAACPO76VAAAEJUlEQVR4nO2cyyt2URTGt48M3GNAKYkBISm3kkiKEuU6MDA1EiMTpUxIiYEYyED5D8iElMtAidwGShlJGVJukcs3W/Y6dU7e873v8by+5zd6Vqt9ztbT2vvY7947xhjzaQgEf366A+QLmgEEzQCCZgBBM4CgGUDQDCBoBhA0AwiaAUSc34aTk5MqHh4e/la78/NzFa+trYl+e3tTuenpadF3d3ch9jD6YGUAQTOAiDE+V22rq6tVbA9TVVVVKpedne3nFebp6Un07Oysyk1MTIh+fHz09Xw0WBlA0AwgaAYQvucML9LT01W8sLAguqysTOXy8vJ8vWNvb0+0/QlsjDHr6+sqfn5+9vWOoGFlAEEzgIjIMOVFZmamiouKikTPzc2pXGFhoa937O/vq3hqakr0ysqKyn18fPh6RyRgZQBBM4CgGUAEPmd4kZWVpeLe3l7RAwMDKpebm+vrHYeHhyoeHx8Xvbq66uuZ4YKVAQTNAAJqmPKioKBAxfaw1dnZqXLO4c6L9/d30ZubmyrX0tISShf/GVYGEDQDCJoBRNTMGV6UlpaquLu7W8WVlZWim5qaXJ9zdnam4vLyctFBLJuwMoCgGUD8imEqFF5eXlQcF/e1dcy5b6u5uVn09vZ2RPtlDCsDCpoBBM0AwvdeW2TS0tJU3NbWJjo2Nta13e7uroqDmCdsWBlA0AwgfsUwVVJSouKZmRkVNzY2ura193TZPzT9BKwMIGgGEDQDiKidMzo6OkQvLS2pXHJysmu7kZERFS8vL4u+ubkJU+/8wcoAgmYAQTOAiJol9Pz8fBUfHR2Jdh5L3traUrG9cW1+fl7lPj9x/nxWBhA0AwjoT9vExETRi4uLKpeUlCS6p6dH5TY2NiLbsQjBygCCZgBBM4CAnjPGxsZE19fXq9zOzo5o54blaIWVAQTNAOLHh6mUlBTR9/f3Kpeamurazv7URTo+/C+wMoCgGUDQDCACX7Vtb29XcWtrq+jj42OVc96+ZnNyciK6rq5O5Zw3sxUXF4seGhpSuf7+fs/+BgkrAwiaAUQgw5R9GZjzxhu/l3/ZOJ/p/LGpoaFB9Ovrq8p5fT4HDSsDCJoBBM0AIpDlkJycHNEZGRlhf77zwmMv7DN8xhjT19cn+uHhwbWdc4Pb7e2t6IuLi2+/3wtWBhA0A4jA/wO3hyxjjImPjxddU1OjcrW1taKdR8O6urrC0p/r62vRBwcHKmfv53X+V396eip6dHRU5fweP2NlAEEzgKAZQAT+S9/V1ZVr7vLyUsX22QnnkeFQljHsW9sSEhJUzr7hbXBwUOXspRP78kpjjKmoqBDt3CzBOeMXQDOAiJojAf8DrAwgaAYQNAMImgEEzQCCZgBBM4CgGUDQDCBoBhA0AwiaAQTNAIJmAEEzgKAZQNAMIP4C5o/o9XPnuT8AAAAASUVORK5CYII=\" y=\"-9.070647\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m817d6e3242\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.689935\" xlink:href=\"#m817d6e3242\" y=\"108.070647\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(25.508685 122.669085)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"63.988636\" xlink:href=\"#m817d6e3242\" y=\"108.070647\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(57.626136 122.669085)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"99.287338\" xlink:href=\"#m817d6e3242\" y=\"108.070647\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(92.924838 122.669085)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7e43cea6f1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m7e43cea6f1\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m7e43cea6f1\" y=\"46.29792\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 50.097139)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m7e43cea6f1\" y=\"81.596621\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 85.39584)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 108.070647 \nL 26.925 9.234284 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 125.761364 108.070647 \nL 125.761364 9.234284 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 108.070647 \nL 125.761364 108.070647 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 9.234284 \nL 125.761364 9.234284 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 209.543182 108.070647 \nL 308.379545 108.070647 \nL 308.379545 9.234284 \nL 209.543182 9.234284 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p34f0836ec7)\">\n    <image height=\"99\" id=\"imagee45c0c7a5d\" transform=\"scale(1 -1)translate(0 -99)\" width=\"99\" x=\"209.543182\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAGMAAABjCAYAAACPO76VAAAEjklEQVR4nO2cSyi9XRTG9/kSSTEwIgMGpi7JJbmUoczkWgyZuORaCiNDhQykZCDlXiQkRcpt4FLGRu6SgUsK4Zut/1r765yO73vf96z//3t+o2f1dM7ePK13v2fv9xyfMebbABX8FeoJgF8gDEUgDEUgDEUgDEUgDEUgDEUgDEUgDEUgDEUgDEUgDEUgDEWEeTFIXFwc6fr6euF1d3eT9vl8wvv+/rWhPDQ0JLyRkRHSZ2dnjswz1KAzFIEwFIEwFOEzLpz0JSQkiHpubo50Zmam/8kEWDNsHh4eSE9PTwuvubk5qHlqA52hCIShCFcuU2VlZaKempoK6nV3d3eifnp6Ip2cnBz0+Pv7+6Le2toi3dvbG/T7eA06QxEIQxEIQxGebIdwPj8/RT06Okp6fHxcePz2NScnR3g1NTWkU1NThZebmytqfqttrz18m+Xg4CDg3N0GnaEIhKEIVy5T9idpXt/e3gqvqakpqPe8uLgQNf9Ub9PS0iLqnp4e0uXl5cL7+PggjcsUIBCGIhCGIlxZM+zdVl4vLi66MaRgYGBA1Hw7pKOjQ3jV1dWko6KihFdaWurC7PyDzlAEwlCE57u29i1qUlKS08MHJCIiQtT8gYiuri7hXV1dka6rqxPe+vq643NDZygCYSgCYSjC811b+5rNH3C7ublxffy3tzdR9/X1kQ4Lk/+Ozs5O0m1tbcLb3d0l/fLy4sjc0BmKQBiKcOXWtqSkRNSzs7Okw8PDhdfe3k56cHDQ6an8J/hBmL2rwJ/1bWxsdGQ8dIYiEIYiEIYiXFkzbPb29khnZ2cLb3t7m3RRUZHbU/kRfJ34+voSHv+b8vPzHRkPnaEIhKEIhKEIT7ZDWltbSe/s7AivoKCAdGFhofD4ehIK+DoR6LsiToHOUATCUIQnl6mTkxPSq6urwisuLia9tLQkvIqKCtKHh4fCi4yMJH15eenIPEMNOkMRCEMRCEMRnqwZ/HSNb5kbY8zz8zPpyspK4U1MTJC2TwH5Cd38/Lwj8/wJ5+fnjr8nOkMRCEMRnuzaBiItLY305uam8KKjo/2+bm1tjXRVVZXw7E/yKSkppMfGxoR3f3/vdwx+0vf6+up3jOPjY7/v8RPQGYpAGIpAGIoI+ZrBycjIEPXGxgbpmJgYv6+zd3eXl5dF3d/fT/r6+lp4/EmWxMRE4fHvkszMzAjPvg13AnSGIhCGIjx/1jYQR0dHol5ZWSFt375y+AGVMf+8teUHQ/zZXmOMSU9PJ81/dcEYHC79r0EYikAYilC1ZtjwHyTmv75jjDGTk5Ok7R+oDIS9dcG/W2GvNZzT09Ogx/i3oDMUgTAUoeoT+E/IysoivbCwILz4+HhR89tSfphljDHv7++kY2NjhZeXl0favrzx1zkFOkMRCEMRCEMRv+2aweGnhcb87MSQ8/j4KGp7DXEbdIYiEIYi/ojLlI19SNXQ0EC6trZWePzZrOHhYeE59aBBsKAzFIEwFIEwFPFHrhm/K+gMRSAMRSAMRSAMRSAMRSAMRSAMRSAMRSAMRSAMRSAMRSAMRSAMRfwN+c44Y7Lb2NEAAAAASUVORK5CYII=\" y=\"-9.070647\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.308117\" xlink:href=\"#m817d6e3242\" y=\"108.070647\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(208.126867 122.669085)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.606818\" xlink:href=\"#m817d6e3242\" y=\"108.070647\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(240.244318 122.669085)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"281.905519\" xlink:href=\"#m817d6e3242\" y=\"108.070647\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 20 -->\n      <g transform=\"translate(275.543019 122.669085)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"209.543182\" xlink:href=\"#m7e43cea6f1\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(196.180682 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"209.543182\" xlink:href=\"#m7e43cea6f1\" y=\"46.29792\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10 -->\n      <g transform=\"translate(189.818182 50.097139)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"209.543182\" xlink:href=\"#m7e43cea6f1\" y=\"81.596621\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 20 -->\n      <g transform=\"translate(189.818182 85.39584)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 209.543182 108.070647 \nL 209.543182 9.234284 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 308.379545 108.070647 \nL 308.379545 9.234284 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 209.543182 108.070647 \nL 308.379545 108.070647 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 209.543182 9.234284 \nL 308.379545 9.234284 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 26.925 226.674284 \nL 125.761364 226.674284 \nL 125.761364 127.83792 \nL 26.925 127.83792 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p477e144a45)\">\n    <image height=\"99\" id=\"imagebae4aab6ce\" transform=\"scale(1 -1)translate(0 -99)\" width=\"99\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAGMAAABjCAYAAACPO76VAAADg0lEQVR4nO2cv0vrYBSGvxbxx1DBSVBHFUHQQRx0VtDRWQcpouLkUHDqv1EEoYPd7OJSN0FdVOygFmrbQUEF6SLaoV1E73bMybWXNk2a95r3mc7hYHLk8XxfmsaEjDFfhkAQ9rsB8g1lAEEZQFAGEJQBBGUAQRlAUAYQlAEEZQDR4XcD7WZ1dVXlyWRS4r29PVXb2NhoR0sCJwMIygAicMvU4uKiykOhkE+d/A0nAwjKAIIygAjcnmHn6wvni05OBhCUAUTglykr5+fnvp6fkwEEZQBBGUAEfs+w3g75/Pz0sRNOBhSUAUTglynrJ/BSqeRjJ5wMKCgDCMoAgjKAoAwgKAOIwF/aWllaWlL5xcVFW8/PyQCCMoCgDCBCxuG/Hu/s7Kh8dna25WYODw9Vfnd3J7Fb6/f6+rrKd3d3Jb6/v1e14eFhV87ZKJwMICgDiKaWqUQiIbH9cfm3tzeJHx8fG25gYmLiuxnbc68fHx8SV6tVVcvn8xJfXl6qWjabVfnp6anE5XJZ1Z6fnyXu6+tTtc7Ozn/27jacDCAoAwjKAKKp2yE9PT0S29f3zc1NidPpdMPHnJycrHvMubk5iYeGhlTNup5Ho1FVW1tbU3mtVpPYvr90d3dLHA7rv82xsTGJC4XCz7+Ai3AygKAMICgDCNduob++vjr6uZubm7q16+vrho6xtbWl8t7eXpUvLCxIvLKyompdXV0S2/eMqakpiblnBAzKAMK1ZWp6elri4+Njtw7riEqlovKDg4MfY2OMSaVSEi8vL6taPB6XOJPJqJr19o9bcDKAoAwgKAOIpvaMYrFYtzY+Pt5yM34Qi8Uktt5+McaY0dFRiSORiKpxz/jlUAYQTS1TR0dHElsv+/5nrN/8PTw8qFp/f7/Eg4ODqvb09OR6L5wMICgDCMoAoqk94/b2VmL7Uxa/gff397q1+fl5lXvxUDQnAwjKAMLxXVu/3ybgBfYHGayXryMjI56fn5MBBGUAQRlAON4z9vf3Vd6ONdVrXl5eVG59N/rMzIyqefGAGycDCMoAwvEydXJyovJcLtdqL75jv1y33sUdGBhQNS/eh8vJAIIygKAMIBzvGWdnZ272AcnV1ZXE29vbnp+PkwEEZQDh+A0JxH04GUBQBhCUAQRlAEEZQFAGEJQBBGUAQRlAUAYQlAEEZQBBGUBQBhCUAQRlAEEZQPwBgPrAQHML2WEAAAAASUVORK5CYII=\" y=\"-127.674284\"/>\n   </g>\n   <g id=\"matplotlib.axis_5\">\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.689935\" xlink:href=\"#m817d6e3242\" y=\"226.674284\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0 -->\n      <g transform=\"translate(25.508685 241.272721)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"63.988636\" xlink:href=\"#m817d6e3242\" y=\"226.674284\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 10 -->\n      <g transform=\"translate(57.626136 241.272721)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"99.287338\" xlink:href=\"#m817d6e3242\" y=\"226.674284\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 20 -->\n      <g transform=\"translate(92.924838 241.272721)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_6\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m7e43cea6f1\" y=\"129.602855\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 133.402074)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m7e43cea6f1\" y=\"164.901556\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 168.700775)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m7e43cea6f1\" y=\"200.200258\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 203.999476)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 26.925 226.674284 \nL 26.925 127.83792 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 125.761364 226.674284 \nL 125.761364 127.83792 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 26.925 226.674284 \nL 125.761364 226.674284 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 26.925 127.83792 \nL 125.761364 127.83792 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 209.543182 226.674284 \nL 308.379545 226.674284 \nL 308.379545 127.83792 \nL 209.543182 127.83792 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p720c7a4ff0)\">\n    <image height=\"99\" id=\"imageb6bc8d3637\" transform=\"scale(1 -1)translate(0 -99)\" width=\"99\" x=\"209.543182\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAGMAAABjCAYAAACPO76VAAADQUlEQVR4nO2cv0tyYRTHH+MdBAcRgkRDQhocHJrDRXQR3IzcQsvBP0IQa21zcGhIXNTBwdlBcfJPqCGqrZwUHFyqd3ve59wXo+Le6zf9fqZzOF468Ok8jz+eez1KqQ9FINhZdwPkH5QBBGUAQRlAUAYQlAEEZQBBGUBQBhCUAcQft/9gMBgUebfb1XEikRC1x8dHHR8eHjrbGACcDCAoAwjXl6lYLCby4+Njt1uAhZMBBGUAQRlAUAYQlAEEZQDh+lvb2Wwm8peXFx2HQiFRC4fDOr6+vha1SqWi4+VyaWOH64OTAQRlAEEZQHjUmg+x3d7e6rhQKIja+/v7yuui0aiOn5+fbe9rHXAygKAMINa+TJm8vb2J/LNl6uzsTMftdtuxntyEkwEEZQBBGUBA7RnT6VTkgUBg5Wsnk4mOM5mMqC0WC3sbcwlOBhCUAQTUMnV0dCTyXq+n40gksvK6fr8v8pOTE1v7cgtOBhCUAQRlAAG1Z1hpNBo6LpVKX74uHo+L/P7+3raenISTAQRlAAG9TB0cHOj44eHhy9e1Wi2RF4tFu1pyFE4GEJQBBGUA4fohtu/w9PSk41wuJ2rNZlPHPp9P1MzDCkoptbe3p+PX11f7GrQZTgYQlAEEZQAB/TnjM8rlso7r9bqo7ezI/7HBYKDjfD4vavP53IHufgYnAwjKAOLXLlPmVyXmL4JK/f+LoXkY7urqStQuLy9t7+2ncDKAoAwgKAOIX7tnmOzu7orc+pWHuWdY7yk8PT3V8XA4tL+5b8DJAIIygNiIZcrKzc2NyM/Pz1e+djQa6TiVSjnV0pfgZABBGUBQBhAbuWdYHzxp3vNnfWClycXFhcitp0ychpMBBGUAsZHLlJV0Oq3jTqcjan6/X8d3d3eils1mdWz9VO/Ek3w4GUBQBhCUAcRW7Bkm1WpV5OYT3ayYBxuSyaSojcdjextTnAwoKAMI6LO2TmB9UJj5FtXr9brdjoCTAQRlAEEZQGzdnmHe16GUUvv7+zqu1WqiZr619Xg8jvalFCcDCsoAYus+gSPDyQCCMoCgDCAoAwjKAIIygKAMICgDCMoAgjKAoAwgKAMIygCCMoCgDCAoAwjKAOIvm8meZYdQHtQAAAAASUVORK5CYII=\" y=\"-127.674284\"/>\n   </g>\n   <g id=\"matplotlib.axis_7\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.308117\" xlink:href=\"#m817d6e3242\" y=\"226.674284\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0 -->\n      <g transform=\"translate(208.126867 241.272721)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.606818\" xlink:href=\"#m817d6e3242\" y=\"226.674284\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 10 -->\n      <g transform=\"translate(240.244318 241.272721)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"281.905519\" xlink:href=\"#m817d6e3242\" y=\"226.674284\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 20 -->\n      <g transform=\"translate(275.543019 241.272721)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_8\">\n    <g id=\"ytick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"209.543182\" xlink:href=\"#m7e43cea6f1\" y=\"129.602855\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0 -->\n      <g transform=\"translate(196.180682 133.402074)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"209.543182\" xlink:href=\"#m7e43cea6f1\" y=\"164.901556\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 10 -->\n      <g transform=\"translate(189.818182 168.700775)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"209.543182\" xlink:href=\"#m7e43cea6f1\" y=\"200.200258\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 20 -->\n      <g transform=\"translate(189.818182 203.999476)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 209.543182 226.674284 \nL 209.543182 127.83792 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 308.379545 226.674284 \nL 308.379545 127.83792 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 209.543182 226.674284 \nL 308.379545 226.674284 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 209.543182 127.83792 \nL 308.379545 127.83792 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pfbfef13a95\">\n   <rect height=\"98.836364\" width=\"98.836364\" x=\"26.925\" y=\"9.234284\"/>\n  </clipPath>\n  <clipPath id=\"p34f0836ec7\">\n   <rect height=\"98.836364\" width=\"98.836364\" x=\"209.543182\" y=\"9.234284\"/>\n  </clipPath>\n  <clipPath id=\"p477e144a45\">\n   <rect height=\"98.836364\" width=\"98.836364\" x=\"26.925\" y=\"127.83792\"/>\n  </clipPath>\n  <clipPath id=\"p720c7a4ff0\">\n   <rect height=\"98.836364\" width=\"98.836364\" x=\"209.543182\" y=\"127.83792\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see that downloading and loading the MNIST dataset is as easy as calling the `mnist.load_data()` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model with Multilayer Perceptrons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do we need a complex model like a convolutional neural network to get the best results with MNIST? You can get good results using a straightforward neural network model with a single hidden layer. This section will create a simple Multilayer Perceptron model that achieves an error rate of approximately 1.73%. We will use this as a baseline for comparison to more complex convolutional neural network models. Let's start by importing the classes and functions we will need."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is always a good idea to initialize the random number generator to a constant to ensure that your script results are reproducible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can load the MNIST dataset using the Keras helper function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training dataset is structured as a 3-dimensional array of instances, image width, and image height. For a Multilayer Perceptron model, we must reduce the images down into a vector of pixels. In this case, the 28 x 28 sized images will be 784-pixel input vectors. We can do this transforms easily using the reshape() function on the NumPy array. The pixel values are integers, so we cast them to floating-point values to normalize them easily in the next step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pixel values are grayscale between 0 and 255. It is almost always a good idea to perform some input values scaling when using neural network models. Because the scale is well known and well behaved, we can quickly normalize the pixel values to the range 0 and 1 by dividing each value by the maximum of 255."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the output variable is an integer from 0 to 9. This is a multiclass classification problem. It is good practice to use the one-hot encoding of the class values, transforming the vector of class integers into a binary matrix. We can easily do this using the built-in `np_utils.to_categorical()` helper function in Keras."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# one hot encode outputs\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now ready to create our simple neural network model. We will define our model in a function. This is handy if you want to extend the example later and try and get a better score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal',\n",
    "    activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784). A rectifier activation function is used for the neurons in the hidden layer. A softmax activation function is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the model's output prediction. Logarithmic loss is used as the loss function (called categorical cross-entropy in Keras), and the efficient ADAM gradient descent algorithm is used to learn the weights. A summary of the network structure is provided below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Summary of Multilayer Perceptron Network Structure](../../images/summary_mlp.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now fit and evaluate the model. The model is fit over ten epochs with updates every 200 images. The test data is used as the validation dataset, allowing you to see the model's skill as it trains. A verbose value of 2 is used to reduce the output to one line for each training epoch. Finally, the test dataset is used to evaluate the model, and a classification error rate is printed.\n",
    "\n",
    "Running the example might take a few minutes when run on a CPU. You should see the output below. This simple network defined in very few lines of code achieves a respectable error rate of 1.78%."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200,\n",
    "verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-11-01 22:53:13.683993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-01 22:53:13.684098: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-01 22:53:13.684175: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (6c3a25e0cdbe): /proc/driver/nvidia/version does not exist\n",
      "2021-11-01 22:53:13.685041: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-01 22:53:14.364143: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2021-11-01 22:53:14.685679: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 8s - loss: 0.2848 - accuracy: 0.9199 - val_loss: 0.1460 - val_accuracy: 0.9567\n",
      "Epoch 2/10\n",
      "300/300 - 4s - loss: 0.1113 - accuracy: 0.9676 - val_loss: 0.1046 - val_accuracy: 0.9691\n",
      "Epoch 3/10\n",
      "300/300 - 4s - loss: 0.0720 - accuracy: 0.9790 - val_loss: 0.0762 - val_accuracy: 0.9770\n",
      "Epoch 4/10\n",
      "300/300 - 4s - loss: 0.0490 - accuracy: 0.9861 - val_loss: 0.0702 - val_accuracy: 0.9784\n",
      "Epoch 5/10\n",
      "300/300 - 5s - loss: 0.0359 - accuracy: 0.9896 - val_loss: 0.0626 - val_accuracy: 0.9807\n",
      "Epoch 6/10\n",
      "300/300 - 4s - loss: 0.0260 - accuracy: 0.9932 - val_loss: 0.0635 - val_accuracy: 0.9803\n",
      "Epoch 7/10\n",
      "300/300 - 6s - loss: 0.0193 - accuracy: 0.9952 - val_loss: 0.0653 - val_accuracy: 0.9798\n",
      "Epoch 8/10\n",
      "300/300 - 9s - loss: 0.0144 - accuracy: 0.9967 - val_loss: 0.0605 - val_accuracy: 0.9804\n",
      "Epoch 9/10\n",
      "300/300 - 5s - loss: 0.0102 - accuracy: 0.9981 - val_loss: 0.0592 - val_accuracy: 0.9810\n",
      "Epoch 10/10\n",
      "300/300 - 4s - loss: 0.0072 - accuracy: 0.9991 - val_loss: 0.0602 - val_accuracy: 0.9814\n",
      "Baseline Error: 1.86%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Convolutional Neural Network for MNIST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have seen how to load the MNIST dataset and train a simple Multilayer Perceptron model, it is time to develop a more sophisticated convolutional neural network or CNN model. Keras does provide a lot of capability for creating convolutional neural networks. This section will create a simple CNN for MNIST that demonstrates how to use all aspects of a modern CNN implementation, including Convolutional layers, Pooling layers, and Dropout layers. The first step is to import the classes and functions needed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import numpy\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, we always initialize the random number generator to a constant seed value for reproducibility of results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need to load the MNIST dataset and reshape it so that it is suitable for use training a CNN. In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions `[samples][channels][width][height]`. Note, we are forcing so-called channels-first ordering for consistency in this example. In the case of RGB, the first dimension channels would be 3 for the red, green, and blue components, and it would be like having three image inputs for every color image. In MNIST, where the channel values are grayscale, the pixel dimension is set to 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples]width][height][channels]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As before, it is a good idea to normalize the pixel values to the range 0 and 1, and one hot encode the output variable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we define our neural network model. Convolutional neural networks are more complex than standard Multilayer Perceptrons, so we will start by using a simple structure, to begin with, that uses all of the elements for state-of-the-art results. Below summarizes the network architecture.\n",
    "\n",
    "1. The first hidden layer is a convolutional layer called a Conv2D. The layer has 32 feature maps, with a size of 5 x 5 and a rectifier activation function. This is the input layer, expecting images with the structure outline above.\n",
    "2. Next, we define a pooling layer that takes the maximum value called MaxPooling2D. It is configured with a pool size of 2 x 2.\n",
    "3. The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer to reduce overfitting.\n",
    "4. Next is a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers.\n",
    "5. Next, a fully connected layer with 128 neurons and rectifier activation function is used.\n",
    "6. Finally, the output layer has ten neurons for the ten classes and a softmax activation function to output probability-like predictions for each class.\n",
    "\n",
    "As before, the model is trained using logarithmic loss and the ADAM gradient descent algorithm. A depiction of the network structure is provided below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Summary of Convolutional Neural Network Structure](../../images/summary_cnn.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We evaluate the model the same way as before with the Multilayer Perceptron. The CNN is fit over ten epochs with a batch size of 200."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running the example, the accuracy on the training and validation test is printed each epoch, and at the end of the classification error rate is printed. Epochs may take about 45 seconds to run on the GPU (e.g., on AWS). You can see that the network achieves an error rate of 1.20%, which is better than our simple Multilayer Perceptron model above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-11-01 22:54:12.709257: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 40s 130ms/step - loss: 0.2478 - accuracy: 0.9286 - val_loss: 0.0888 - val_accuracy: 0.9728\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 33s 109ms/step - loss: 0.0782 - accuracy: 0.9766 - val_loss: 0.0516 - val_accuracy: 0.9833\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 36s 121ms/step - loss: 0.0529 - accuracy: 0.9841 - val_loss: 0.0428 - val_accuracy: 0.9857\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.0429 - accuracy: 0.9866 - val_loss: 0.0383 - val_accuracy: 0.9870\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 39s 129ms/step - loss: 0.0347 - accuracy: 0.9890 - val_loss: 0.0379 - val_accuracy: 0.9870\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 36s 119ms/step - loss: 0.0292 - accuracy: 0.9907 - val_loss: 0.0377 - val_accuracy: 0.9861\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 33s 109ms/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0317 - val_accuracy: 0.9891\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 33s 111ms/step - loss: 0.0203 - accuracy: 0.9938 - val_loss: 0.0330 - val_accuracy: 0.9885\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 37s 124ms/step - loss: 0.0175 - accuracy: 0.9942 - val_loss: 0.0342 - val_accuracy: 0.9895\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 31s 103ms/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 0.0375 - val_accuracy: 0.9890\n",
      "CNN Error: 1.10%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Larger Convolutional Neural Network for MNIST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have seen how to create a simple CNN, let's take a look at a model capable of close to state-of-the-art results. We import the classes and functions then load and prepare the data the same as in the previous CNN example. This time we define a larger CNN architecture with additional convolutional, max-pooling layers and fully connected layers. The network topology can be summarized as follows.\n",
    "\n",
    "1. Convolutional layer with 30 feature maps of size 5 x 5.\n",
    "2. Pooling layer taking the max over 2 x 2 patches.\n",
    "3. Convolutional layer with 15 feature maps of size 3 x 3.\n",
    "4. Pooling layer taking the max over 2 x 2 patches.\n",
    "5. Dropout layer with a probability of 20%.\n",
    "6. Flatten layer.\n",
    "7. Fully connected layer with 128 neurons and rectifier activation.\n",
    "8. Fully connected layer with 50 neurons and rectifier activation.\n",
    "9. Output layer.\n",
    "\n",
    "A depiction of this larger network structure is provided below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Summary of the Larger Convolutional Neural Network Structure](../../images/summary_cnn_larger.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Like the previous two experiments, the model fits over ten epochs with a batch size of 200."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# define the larger model\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running the example prints accuracy on each epoch's training and validation datasets and a final classification error rate. The model takes about 60 seconds to run per epoch on a modern CPU. This slightly larger model achieves a respectable classification error rate of 1.06%."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# build the model\n",
    "model = larger_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-11-01 23:00:39.850256: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 45s 146ms/step - loss: 0.4067 - accuracy: 0.8716 - val_loss: 0.0829 - val_accuracy: 0.9721\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 39s 129ms/step - loss: 0.0992 - accuracy: 0.9695 - val_loss: 0.0518 - val_accuracy: 0.9838\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 40s 134ms/step - loss: 0.0742 - accuracy: 0.9773 - val_loss: 0.0432 - val_accuracy: 0.9863\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 41s 137ms/step - loss: 0.0584 - accuracy: 0.9818 - val_loss: 0.0432 - val_accuracy: 0.9867\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 45s 148ms/step - loss: 0.0501 - accuracy: 0.9847 - val_loss: 0.0298 - val_accuracy: 0.9898\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.0445 - accuracy: 0.9858 - val_loss: 0.0292 - val_accuracy: 0.9909\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 44s 146ms/step - loss: 0.0403 - accuracy: 0.9877 - val_loss: 0.0269 - val_accuracy: 0.9916\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 44s 147ms/step - loss: 0.0353 - accuracy: 0.9888 - val_loss: 0.0286 - val_accuracy: 0.9900\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 43s 143ms/step - loss: 0.0326 - accuracy: 0.9896 - val_loss: 0.0250 - val_accuracy: 0.9914\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 44s 148ms/step - loss: 0.0282 - accuracy: 0.9907 - val_loss: 0.0256 - val_accuracy: 0.9906\n",
      "Large CNN Error: 0.94%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extension Ideas to Improve the Model\n",
    "\n",
    "This is not an optimized network topology. Nor is this a reproduction of network topology from a recent paper. There is much opportunity for you to tune and improve upon this model. What is the best classification error rate you can achieve?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this lesson, you discovered the MNIST handwritten digit recognition problem and deep learning models developed in Python using the Keras library to achieve excellent results. Working through this tutorial, you learned:\n",
    "\n",
    "* How to load the MNIST dataset in Keras and generate plots of the dataset.\n",
    "* How to reshape the MNIST dataset and develop a simple but well-performing Multilayer Perceptron model for the problem.\n",
    "* How to use Keras to create convolutional neural network models for MNIST.\n",
    "* How to develop and evaluate larger CNN models for MNIST capable of near world-class results."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "d123ce8976c88e78ee3c18bcbbfaab0f3a0fc8ee1344c6ddf84e1b83b40ff96b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}