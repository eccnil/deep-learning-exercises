{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Deep-Learning-Challenge/challenge-notebooks/blob/master/1.Multilayer%20Perceptrons/1.Lessons/3.%20Use%20Models%20With%20Scikit-Learn%20For%20Fine%20Tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Models With Scikit-Learn For Fine Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library is the most popular library for general machine learning in Python. This lesson will explore how you can use deep learning models from Keras with the scikit-learn library in Python. After completing this lesson, you will know:\n",
    "\n",
    "* How to wrap a Keras model for use with the scikit-learn machine learning library.\n",
    "* How to easily evaluate Keras models using cross-validation in scikit-learn.\n",
    "* How to tune Keras model hyperparameters using grid search in scikit-learn.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a popular library for deep learning in Python, but its focus is deep learning, not machine learning. It strives for minimalism, focusing on only what you need to define and build deep learning models quickly. The scikit-learn library in Python is built upon the SciPy stack for efficient numerical computation. It is a fully-featured library for general-purpose machine learning and provides many utilities useful in developing deep learning models. Not least:\n",
    "\n",
    "* Evaluation of models using resampling methods like k-fold cross-validation.\n",
    "* Efficient search and evaluation of model hyperparameters.\n",
    "\n",
    "The Keras library provides a convenient wrapper for deep learning models for\n",
    "classification or regression estimators in scikit-learn. In the next sections, we will work through examples of using the `KerasClassifier` wrapper for a classification neural network created in Keras and utilized in the scikit-learn library. The test problem is the Pima Indians onset of diabetes classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../datasets/pima-indians-diabetes.data.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install numpy keras tensorflow sklearn\n",
    "#!conda install --yes --prefix {sys.prefix} numpy keras tensorflow sklearn\n",
    "\n",
    "dataset_name = \"pima-indians-diabetes.data.csv\"\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATASET = f\"https://github.com/Deep-Learning-Challenge/challenge-notebooks/raw/master/datasets/{dataset_name}\"\n",
    "else:\n",
    "    DATASET = f\"../../datasets/{dataset_name}\"\n",
    "    \n",
    "DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `KerasClassifier` and `KerasRegressor` classes in Keras take an argument `build_fn`, which is the name of the function to call to create your model. You must define a function called whatever you like that defines your model, compiles it, and returns it. In the example below, we define a function `create_model()` that creates a simple multilayer neural network for the problem.\n",
    "\n",
    "We pass this function name to the `KerasClassifier` class by the `build_fn` argument. We also pass in additional arguments of `epochs=150` and `batch_size=10`. These are automatically bundled up and passed on to the `fit()` function, which is called internally by the `KerasClassifier` class. In this example, we use the scikit-learn `StratifiedKFold` to perform 10-fold stratified cross-validation. This is a resampling technique that can provide a robust estimate of a machine learning model's performance on unseen data. We use the scikit-learn function `cross_val_score()` to evaluate our model using the cross-validation scheme and print the results.\n",
    "\n",
    "Running the example displays the skill of the model for each epoch. A total of 10 models are created and evaluated and the final average accuracy is displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that when the Keras model is wrapped that estimating model accuracy can be greatly streamlined, compared to the manual enumeration of cross-validation folds performed in the previous lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Deep Learning Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example showed how easy it is to wrap your deep learning model from Keras and use it in functions from the scikit-learn library. In this example, we go a step further. We already know we can provide arguments to the `fit()` function. The function that we specify to the `build_fn` argument when creating the `KerasClassifier` wrapper can also take arguments. We can use these arguments to customize the construction of the model further.\n",
    "\n",
    "In this example, we use a grid search to evaluate different configurations for our neural network model and report on the combination that provides the best-estimated performance. The `create_model()` function is d to take two arguments, `optimizer` and `init`, which must have default values. This will allow us to evaluate using different optimization algorithms and weight initialization schemes for our network. After creating our model, we define arrays of values for the parameter we wish to search, specifically:\n",
    "\n",
    "* Optimizers for exploring different weight values.\n",
    "* Initializers for preparing the network weights using different schemes.\n",
    "* The number of epochs for training the model for the different number of exposures to the training dataset.\n",
    "* Batches for varying the number of samples before weight updates.\n",
    "\n",
    "The options are specified into a dictionary and passed to the configuration of the `GridSearchCV` scikit-learn class. This class will evaluate a version of our neural network model for each combination of parameters (2 x 3 x 3 x 3) for the combinations of optimizers, initializations, epochs, and batches). Each combination is then evaluated using the default of 3-fold stratified\n",
    "cross-validation.\n",
    "\n",
    "That is a lot of models and a lot of computation. This is not a scheme that you want to use lightly because of its time to compute. It may be useful for you to design small experiments with a smaller subset of your data that will complete in a reasonable time. This experiment is reasonable in this case because of the small network and the small dataset (less than 1,000 instances and nine attributes). Finally, the performance and combination of configurations for the best model are displayed, followed by the performance of all combinations of parameters.\n",
    "\n",
    "This might take about 5 minutes to complete on your workstation executed on the CPU. Running the example shows the results below. We can see that the grid search discovered that using a uniform initialization scheme, adam optimizer, 150 epochs and a batch size of 5 achieved the best cross-validation score of approximately 75% on this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 22:44:31.932874: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-06 22:44:31.933205: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-06 22:44:31.933920: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-06 22:44:31.949499: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-06 22:44:32.114557: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-06 22:44:32.119354: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-06 22:44:32.126676: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-06 22:44:32.132316: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "123/123 [==============================] - 2s 4ms/step - loss: 7.3192 - accuracy: 0.5415\n",
      " 83/123 [===================>..........] - ETA: 0s - loss: 12.9827 - accuracy: 0.6000Epoch 2/50\n",
      "123/123 [==============================] - 3s 4ms/step - loss: 10.3877 - accuracy: 0.5912\n",
      "Epoch 2/50\n",
      "123/123 [==============================] - 3s 4ms/step - loss: 1.9888 - accuracy: 0.6531\n",
      "Epoch 2/50\n",
      "123/123 [==============================] - 3s 3ms/step - loss: 3.0685 - accuracy: 0.5342\n",
      "Epoch 2/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 1.9962 - accuracy: 0.5626\n",
      "Epoch 3/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 2.6841 - accuracy: 0.5765\n",
      "Epoch 3/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.8430 - accuracy: 0.6124\n",
      "Epoch 3/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.8612 - accuracy: 0.5407\n",
      "Epoch 3/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 1.4697 - accuracy: 0.5740\n",
      "Epoch 4/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 1.1340 - accuracy: 0.5326\n",
      "Epoch 4/50\n",
      " 23/123 [====>.........................] - ETA: 0s - loss: 1.4627 - accuracy: 0.5913123/123 [==============================] - 1s 5ms/step - loss: 0.7647 - accuracy: 0.6319\n",
      "Epoch 4/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.7282 - accuracy: 0.6042\n",
      " 20/123 [===>..........................] - ETA: 0s - loss: 0.7589 - accuracy: 0.6300Epoch 4/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 1.1865 - accuracy: 0.6049\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.8668 - accuracy: 0.5358\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.7218 - accuracy: 0.6547\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6944 - accuracy: 0.6075\n",
      " 20/123 [===>..........................] - ETA: 0s - loss: 0.7659 - accuracy: 0.5400Epoch 5/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 1.0554 - accuracy: 0.5902\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.8010 - accuracy: 0.5521\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.7086 - accuracy: 0.6580\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6826 - accuracy: 0.6238\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 1.0191 - accuracy: 0.5724\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.7499 - accuracy: 0.6107\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6711 - accuracy: 0.6319\n",
      " 22/123 [====>.........................] - ETA: 0s - loss: 0.6565 - accuracy: 0.6182Epoch 7/50\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.6910 - accuracy: 0.6645\n",
      " 16/123 [==>...........................] - ETA: 0s - loss: 0.6423 - accuracy: 0.6250Epoch 7/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.9279 - accuracy: 0.6146\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.7211 - accuracy: 0.6238\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6631 - accuracy: 0.6450\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6790 - accuracy: 0.6596\n",
      "  1/123 [..............................] - ETA: 0s - loss: 0.4718 - accuracy: 0.8000Epoch 8/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8648 - accuracy: 0.6146\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7071 - accuracy: 0.6531\n",
      " 94/123 [=====================>........] - ETA: 0s - loss: 0.6434 - accuracy: 0.7043Epoch 9/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6547\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6611 - accuracy: 0.6922\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.8333 - accuracy: 0.6146\n",
      "Epoch 10/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.6547\n",
      "Epoch 10/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6451 - accuracy: 0.6450\n",
      " 43/123 [=========>....................] - ETA: 0s - loss: 0.7985 - accuracy: 0.6326Epoch 10/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6513 - accuracy: 0.6938\n",
      "Epoch 10/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.8196 - accuracy: 0.6341\n",
      " 67/123 [===============>..............] - ETA: 0s - loss: 0.6710 - accuracy: 0.7015Epoch 11/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6735 - accuracy: 0.6759\n",
      " 85/123 [===================>..........] - ETA: 0s - loss: 0.6638 - accuracy: 0.7035Epoch 11/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6824\n",
      " 56/123 [============>.................] - ETA: 0s - loss: 0.8115 - accuracy: 0.6286 - ETA: 0s - loss: 0.6406 - accuracy: 0.6762Epoch 11/50\n",
      "  1/123 [..............................] - 0s 3ms/step - loss: 0.6547 - accuracy: 0.6889\n",
      " - ETA: 0s - loss: 0.9057 - accuracy: 0.4000Epoch 11/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.7923 - accuracy: 0.6146\n",
      "Epoch 12/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6588 - accuracy: 0.6840\n",
      "Epoch 12/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6330 - accuracy: 0.6938\n",
      "Epoch 12/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6430 - accuracy: 0.7068\n",
      "Epoch 12/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.7875 - accuracy: 0.6423\n",
      " 93/123 [=====================>........] - ETA: 0s - loss: 0.6603 - accuracy: 0.6602Epoch 13/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6724 - accuracy: 0.6498\n",
      "Epoch 13/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6323 - accuracy: 0.6743\n",
      "Epoch 13/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6255 - accuracy: 0.6971\n",
      "Epoch 13/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7550 - accuracy: 0.6130\n",
      "Epoch 14/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6404 - accuracy: 0.6710\n",
      "Epoch 14/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6743\n",
      "115/123 [===========================>..] - ETA: 0s - loss: 0.6150 - accuracy: 0.7304Epoch 14/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.7264\n",
      "Epoch 14/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7522 - accuracy: 0.6407\n",
      "Epoch 15/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6218 - accuracy: 0.6987\n",
      "Epoch 15/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6353 - accuracy: 0.6873\n",
      "Epoch 15/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6130 - accuracy: 0.6938\n",
      "Epoch 15/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7507 - accuracy: 0.6374\n",
      "Epoch 16/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6359 - accuracy: 0.6857\n",
      "Epoch 16/50\n",
      " - ETA: 11s - loss: 0.4696 - accuracy: 0.6000123/123 [==============================] - 1s 6ms/step - loss: 0.6181 - accuracy: 0.6906\n",
      "Epoch 16/50\n",
      "123/123 [==============================] - 1s 6ms/step - loss: 0.6123 - accuracy: 0.7052\n",
      "Epoch 16/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7658 - accuracy: 0.6033\n",
      "Epoch 17/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6922\n",
      "Epoch 17/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6840\n",
      "Epoch 17/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6167 - accuracy: 0.7020\n",
      "Epoch 17/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.7449 - accuracy: 0.6553\n",
      " 74/123 [=================>............] - ETA: 0s - loss: 0.6217 - accuracy: 0.6784Epoch 18/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6743\n",
      "Epoch 18/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6066 - accuracy: 0.6922\n",
      "Epoch 18/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6016 - accuracy: 0.7248\n",
      "Epoch 18/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7281 - accuracy: 0.6455\n",
      "Epoch 19/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6182 - accuracy: 0.6840\n",
      "Epoch 19/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6218 - accuracy: 0.6873\n",
      "Epoch 19/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5985 - accuracy: 0.7182\n",
      " 98/123 [======================>.......] - ETA: 0s - loss: 0.7148 - accuracy: 0.6571Epoch 19/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.7215 - accuracy: 0.6602\n",
      "Epoch 20/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6292 - accuracy: 0.6857\n",
      "Epoch 20/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6027 - accuracy: 0.6954\n",
      "Epoch 20/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6072 - accuracy: 0.7215\n",
      "Epoch 20/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.7332 - accuracy: 0.6520\n",
      "Epoch 21/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6130 - accuracy: 0.6857\n",
      "Epoch 21/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5981 - accuracy: 0.6906\n",
      "Epoch 21/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5954 - accuracy: 0.7248\n",
      "Epoch 21/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.6715\n",
      "Epoch 22/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6098 - accuracy: 0.6808\n",
      "Epoch 22/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6035 - accuracy: 0.6906\n",
      "Epoch 22/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5892 - accuracy: 0.7378\n",
      "Epoch 22/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.6569\n",
      "Epoch 23/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6011 - accuracy: 0.6840\n",
      "Epoch 23/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5992 - accuracy: 0.6971\n",
      "Epoch 23/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5987 - accuracy: 0.7231\n",
      " 32/123 [======>.......................] - ETA: 0s - loss: 0.5718 - accuracy: 0.6875Epoch 23/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.7184 - accuracy: 0.6683\n",
      "Epoch 24/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5949 - accuracy: 0.6987\n",
      " 26/123 [=====>........................] - ETA: 0s - loss: 0.7008 - accuracy: 0.7154Epoch 24/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5875 - accuracy: 0.6873\n",
      "Epoch 24/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5995 - accuracy: 0.7068\n",
      "Epoch 24/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.7107 - accuracy: 0.6862\n",
      "Epoch 25/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5985 - accuracy: 0.6954\n",
      "Epoch 25/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5888 - accuracy: 0.7134\n",
      "Epoch 25/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.7313\n",
      "Epoch 25/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6839 - accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5966 - accuracy: 0.6938\n",
      "Epoch 26/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5836 - accuracy: 0.6775\n",
      " 30/123 [======>.......................] - ETA: 0s - loss: 0.6326 - accuracy: 0.6933Epoch 26/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5852 - accuracy: 0.7248\n",
      "Epoch 26/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.6943\n",
      " 89/123 [====================>.........] - ETA: 0s - loss: 0.5825 - accuracy: 0.7146Epoch 27/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5943 - accuracy: 0.7085\n",
      "Epoch 27/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5850 - accuracy: 0.7003\n",
      "Epoch 27/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5862 - accuracy: 0.7313\n",
      "Epoch 27/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6947 - accuracy: 0.6715\n",
      " 54/123 [============>.................] - ETA: 0s - loss: 0.5687 - accuracy: 0.7333Epoch 28/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5924 - accuracy: 0.7020\n",
      "Epoch 28/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5775 - accuracy: 0.6971\n",
      "Epoch 28/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5725 - accuracy: 0.7410\n",
      "Epoch 28/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6950 - accuracy: 0.6667\n",
      "Epoch 29/50\n",
      " - 0s 3ms/step - loss: 0.5867 - accuracy: 0.7036\n",
      " 31/123 [======>.......................] - ETA: 0s - loss: 0.7221 - accuracy: 0.7032Epoch 29/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5790 - accuracy: 0.7166\n",
      "Epoch 29/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5772 - accuracy: 0.7280\n",
      " 35/123 [=======>......................] - ETA: 0s - loss: 0.6026 - accuracy: 0.6857Epoch 29/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6938 - accuracy: 0.6862\n",
      "Epoch 30/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5773 - accuracy: 0.7052\n",
      "Epoch 30/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5869 - accuracy: 0.7068\n",
      "111/123 [==========================>...] - ETA: 0s - loss: 0.5759 - accuracy: 0.7550Epoch 30/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5740 - accuracy: 0.7476\n",
      "Epoch 30/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6740 - accuracy: 0.7057\n",
      "Epoch 31/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5902 - accuracy: 0.6954\n",
      "Epoch 31/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5739 - accuracy: 0.7052\n",
      "Epoch 31/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5793 - accuracy: 0.7231\n",
      "Epoch 31/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6772 - accuracy: 0.6748\n",
      "Epoch 32/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5878 - accuracy: 0.6808\n",
      "Epoch 32/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5731 - accuracy: 0.7378\n",
      "Epoch 32/\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5729 - accuracy: 0.7101\n",
      "Epoch 32/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6828 - accuracy: 0.6780\n",
      "Epoch 33/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5780 - accuracy: 0.6889\n",
      " 77/123 [=================>............] - ETA: 0s - loss: 0.5802 - accuracy: 0.7039Epoch 33/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5698 - accuracy: 0.7362\n",
      "Epoch 33/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5820 - accuracy: 0.7166\n",
      "Epoch 33/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6626 - accuracy: 0.6943\n",
      "Epoch 34/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5849 - accuracy: 0.7117\n",
      "Epoch 34/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5800 - accuracy: 0.7231\n",
      "Epoch 34/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5727 - accuracy: 0.7150\n",
      "Epoch 34/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6633 - accuracy: 0.6797\n",
      "Epoch 35/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5771 - accuracy: 0.7085\n",
      "Epoch 35/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5767 - accuracy: 0.6938\n",
      "Epoch 35/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5485 - accuracy: 0.7557\n",
      "Epoch 35/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6580 - accuracy: 0.6878\n",
      "Epoch 36/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5762 - accuracy: 0.6889\n",
      "Epoch 36/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5742 - accuracy: 0.7085\n",
      "Epoch 36/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5692 - accuracy: 0.7427\n",
      "Epoch 36/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6762 - accuracy: 0.6634\n",
      " 67/123 [===============>..............] - ETA: 0s - loss: 0.5700 - accuracy: 0.7015Epoch 37/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5837 - accuracy: 0.6906\n",
      "Epoch 37/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5610 - accuracy: 0.7036\n",
      "Epoch 37/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5602 - accuracy: 0.7427\n",
      "Epoch 37/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6631 - accuracy: 0.6797\n",
      " 60/123 [=============>................] - ETA: 0s - loss: 0.5631 - accuracy: 0.7000Epoch 38/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5601 - accuracy: 0.7134\n",
      "Epoch 38/50\n",
      " 84/123 [===================>..........] - ETA: 0s - loss: 0.6420 - accuracy: 0.7190 - 0s 3ms/step - loss: 0.5750 - accuracy: 0.6987\n",
      "Epoch 38/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5609 - accuracy: 0.7182\n",
      " 20/123 [===>..........................] - ETA: 0s - loss: 0.5730 - accuracy: 0.6800Epoch 38/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6429 - accuracy: 0.7008\n",
      "Epoch 39/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5723 - accuracy: 0.7101\n",
      "Epoch 39/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5686 - accuracy: 0.7036\n",
      "Epoch 39/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5426 - accuracy: 0.7459\n",
      "Epoch 39/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6613 - accuracy: 0.6748\n",
      "Epoch 40/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5550 - accuracy: 0.7085\n",
      "Epoch 40/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5761 - accuracy: 0.7052\n",
      "Epoch 40/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5609 - accuracy: 0.7410\n",
      "Epoch 40/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6427 - accuracy: 0.6927\n",
      "Epoch 41/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5576 - accuracy: 0.7264\n",
      "Epoch 41/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5714 - accuracy: 0.7036\n",
      "Epoch 41/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5460 - accuracy: 0.7541\n",
      "Epoch 41/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6265 - accuracy: 0.7073\n",
      "Epoch 42/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5697 - accuracy: 0.7036\n",
      "Epoch 42/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5679 - accuracy: 0.7052\n",
      "Epoch 42/50\n",
      "103/123 [========================>.....]123/123 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7492\n",
      " 43/123 [=========>....................] - ETA: 0s - loss: 0.5121 - accuracy: 0.7302 - ETA: 0s - loss: 0.6567 - accuracy: 0.6621Epoch 42/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6477 - accuracy: 0.6634\n",
      "Epoch 43/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5484 - accuracy: 0.7182\n",
      "Epoch 43/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5453 - accuracy: 0.7492\n",
      "Epoch 43/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5662 - accuracy: 0.7068\n",
      " 18/123 [===>..........................] - ETA: 0s - loss: 0.5170 - accuracy: 0.7667Epoch 43/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6429 - accuracy: 0.7008\n",
      "Epoch 44/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5704 - accuracy: 0.7134\n",
      "Epoch 44/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5352 - accuracy: 0.7590\n",
      "Epoch 44/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5719 - accuracy: 0.7003\n",
      "Epoch 44/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6450 - accuracy: 0.6894\n",
      "Epoch 45/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5639 - accuracy: 0.7020\n",
      "Epoch 45/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.5499 - accuracy: 0.7492\n",
      "Epoch 45/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5744 - accuracy: 0.7150\n",
      "Epoch 45/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6846\n",
      "Epoch 46/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5640 - accuracy: 0.6954\n",
      "Epoch 46/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5331 - accuracy: 0.7524\n",
      " 97/123 [======================>.......] - ETA: 0s - loss: 0.5799 - accuracy: 0.7052Epoch 46/50\n",
      " - 1s 4ms/step - loss: 0.5647 - accuracy: 0.7150\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6343 - accuracy: 0.6813\n",
      "Epoch 46/50\n",
      " 35/123 [=======>......................] - ETA: 0s - loss: 0.5968 - accuracy: 0.6800Epoch 47/50\n",
      " 59/123 [=============>................]123/123 [==============================] - ETA: 0s - loss: 0.6436 - accuracy: 0.6949 - 0s 4ms/step - loss: 0.5599 - accuracy: 0.7003\n",
      "Epoch 47/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5217 - accuracy: 0.7671\n",
      "Epoch 47/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5701 - accuracy: 0.7085\n",
      "Epoch 47/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.6389 - accuracy: 0.7008\n",
      "Epoch 48/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5637 - accuracy: 0.7068\n",
      "Epoch 48/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5452 - accuracy: 0.7541\n",
      "Epoch 48/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.5636 - accuracy: 0.7101\n",
      "Epoch 48/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6303 - accuracy: 0.6878\n",
      " 64/123 [==============>...............] - ETA: 0s - loss: 0.5554 - accuracy: 0.7125Epoch 49/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5670 - accuracy: 0.7085\n",
      "Epoch 49/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5443 - accuracy: 0.7524\n",
      "Epoch 49/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6328 - accuracy: 0.6992\n",
      "Epoch 50/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.5741 - accuracy: 0.7199\n",
      " 39/123 [========>.....................] - ETA: 0s - loss: 0.5651 - accuracy: 0.7179Epoch 49/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5613 - accuracy: 0.7150\n",
      " 67/123 [===============>..............] - ETA: 0s - loss: 0.6949 - accuracy: 0.6866Epoch 50/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5442 - accuracy: 0.7459\n",
      "Epoch 50/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5613 - accuracy: 0.7166\n",
      "Epoch 50/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.6257 - accuracy: 0.7171\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5555 - accuracy: 0.7020\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7410\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.5658 - accuracy: 0.7101\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 0.8429 - accuracy: 0.5556\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.7361 - accuracy: 0.6169\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.7208\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5928 - accuracy: 0.6948\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "123/123 [==============================] - 2s 4ms/step - loss: 6.6386 - accuracy: 0.6010\n",
      " 70/123 [================>.............] - ETA: 0s - loss: 9.0647 - accuracy: 0.5914 Epoch 2/50\n",
      "123/123 [==============================] - 2s 5ms/step - loss: 3.2764 - accuracy: 0.6124\n",
      " 34/123 [=======>......................] - ETA: 0s - loss: 3.5611 - accuracy: 0.5824Epoch 2/50\n",
      "123/123 [==============================] - 2s 5ms/step - loss: 1.9842 - accuracy: 0.5854\n",
      "Epoch 2/50\n",
      "123/123 [==============================] - 2s 5ms/step - loss: 6.7040 - accuracy: 0.5896\n",
      "Epoch 2/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 3.3631 - accuracy: 0.5961\n",
      " 99/123 [=======================>......] - ETA: 0s - loss: 1.3981 - accuracy: 0.5697Epoch 3/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 1.3182 - accuracy: 0.5707\n",
      " 35/123 [=======>......................] - ETA: 0s - loss: 3.0672 - accuracy: 0.6000Epoch 3/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 1.5698 - accuracy: 0.6531\n",
      " 15/123 [==>...........................] - ETA: 0s - loss: 0.9204 - accuracy: 0.6667Epoch 3/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 2.1022 - accuracy: 0.5505\n",
      "Epoch 3/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 2.6236 - accuracy: 0.6156\n",
      "Epoch 4/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 1.0266 - accuracy: 0.5919\n",
      "Epoch 4/50\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.8901 - accuracy: 0.5795 - 0s 3ms/step - loss: 1.3069 - accuracy: 0.6401\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 1.4223 - accuracy: 0.5554\n",
      "Epoch 4/50\n",
      "  1/123 [..............................] - ETA: 0s - loss: 0.2151 - accuracy: 1.0000Epoch 4/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 2.1132 - accuracy: 0.6352\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.8484 - accuracy: 0.6228\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 1.0398 - accuracy: 0.6270\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 1.2814 - accuracy: 0.5749\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.7972 - accuracy: 0.6146\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 1.6789 - accuracy: 0.6336\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.8636 - accuracy: 0.6515\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 1.1715 - accuracy: 0.5961\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.7311 - accuracy: 0.6650\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 1.4165 - accuracy: 0.6433\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.7962 - accuracy: 0.6547\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 1.0510 - accuracy: 0.5717\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.7083 - accuracy: 0.6520\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 1.2024 - accuracy: 0.6482\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.7566 - accuracy: 0.6319\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.9824 - accuracy: 0.6140\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.7101 - accuracy: 0.6520\n",
      "Epoch 9/50\n",
      " - ETA: 0s - loss: 0.7042 - accuracy: 0.6590123/123 [==============================] - 0s 4ms/step - loss: 1.0355 - accuracy: 0.6612\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6981 - accuracy: 0.6612\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.8540 - accuracy: 0.6450\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.6955 - accuracy: 0.6423\n",
      " 45/123 [=========>....................] - ETA: 0s - loss: 0.8175 - accuracy: 0.6800Epoch 10/50\n",
      " 97/123 [======================>.......] - ETA: 1s - loss: 0.5054 - accuracy: 0.8000 - ETA: 0s - loss: 1.0263 - accuracy: 0.6495"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset with grid search via sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(DATASET, delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "inits = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose = 0, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, you discovered how you could wrap your Keras deep learning models and use them in the scikit-learn general machine learning library. You learned:\n",
    "\n",
    "* Specifically how to wrap Keras models so that they can be used with the scikit-learn machine learning library.\n",
    "* How to use a wrapped Keras model as part of evaluating model performance in scikit-learn.\n",
    "* How to perform hyperparameter tuning in scikit-learn using a wrapped Keras model.\n",
    "\n",
    "You can see that using scikit-learn for standard machine learning operations such as model evaluation and model hyperparameter optimization can save much time over implementing from scratch."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
